# 3. 数据处理

​	这一章试图涵盖所有使用PySpark处理数据的步骤。虽然，这部分内容的相关数据量很小，但处理大数据时，步骤完全一样。数据处理是执行机器学习所需的关键步骤，因为我们需要清理(clean)、过滤(filter)、合并(merge)和转换(transform)数据，以将其转换为所需的形式，以便我们能够训练机器学习模型。我们将使用多个PySpark函数来进行数据处理。



## 3.1 读取数据

​	假定已经安装好Spark 2.3 版本。第一步，创建`SparkSession`

```python
from pyspark.sql import SparkSession
spark=SparkSession.builder.appName('data_processing').getOrCreate()
df=spark.read.csv('sample_data.csv', inferSchema=True, header=True)
```

​	我们需要确保数据文件与打开的Pyspark处于同一文件下，或者指定数据所在文件夹的路径以及数据文件名。我们可以读取多种数据类型的文件，只需要根据文件类型(csv, JSON, parquet, table, text)更新读取格式参数。对于分隔符文件，需要在读取文件的时候额外添加一个参数(`sep='\t'`)。将参数`inferSchema`设置为`true`表示后台的Spark将自行推断数据集中值的数据类型。

## 3.2 增加新列



## 3.3 过滤数据



## 3.4 去重 Distinct Values in Column



## 3.5 数据分组



## 3.6 agg



## 3.7 自定义函数



## 3.8 Drop Duplicate Values



## 3.9 Delete Column



## 3.10 写数据



## 3.11 结论

