# 5 逻辑回归

​	本章重点介绍如何使用`PySpark`构建`Logistic`回归模型，同时理解逻辑回归背后的思想。 `Logistic回归`用于**分类问题**。 我们已经在前面的章节中看到了分类细节。 虽然它用于分类，但仍称为逻辑回归。 这是因为在引擎盖下，线性回归方程仍然可以找到输入变量和目标变量之间的关系。 线性回归和逻辑回归之间的**主要区别**在于我们使用某种非线性函数将后者的输出转换为将其限制在0和1之间的概率。例如，我们可以使用逻辑回归来预测用户是否会购买产品与否。 在这种情况下，模型将返回每个用户的购买概率。 Logistic回归在许多业务应用程序中得到广泛使用。



## 5.1 概率

​	要理解逻辑回归，我们必须首先回顾概率概念。 它被定义为在所有可能的结果上发生期望事件或感兴趣结果的机会。 举一个例子，如果我们翻硬币，头或尾的几率相等（50％）。

​	如果我们滚动一个公平的骰子，那么获得（1到6）之间的任何数字的概率等于16.7％。
​	如果我们从一个包含四个绿色球和一个蓝色球的球中挑选一个球，那么挑选一个绿球的概率是80％。
​	Logistic回归用于预测每个目标类别的概率。 在二进制分类（仅两个类）的情况下，它返回与每个记录的每个类相关联的概率。 如上所述，它在幕后使用线性回归来捕获输入和输出变量之间的关系，但我们还使用一个元素（非线性函数）将输出从连续形式转换为概率。 让我们借助一个例子来理解这一点。 让我们考虑一下，我们必须使用模型来预测某个特定用户是否会购买该产品，而我们只使用一个输入变量，该变量是用户在网站上花费的时间。 



## 5.2 Interpretation (Coefficients)

​	使用称为梯度下降的技术找到输入变量的系数，该技术是寻找优化损失函数的总误差最小化的方式。 

## 5.3 Dummy Variables

​	到目前为止，我们只处理连续/数值变量，但数据集中存在分类变量是不可避免的。 因此，让我们理解使用分类值进行建模的方法。 由于机器学习模型仅以数字格式使用数据，因此我们必须采用某种技术**以数字形式转换分类数据**。 我们已经看到上面的一个例子，我们将目标类（是/否）转换为数值（1或0）。 这称为标签编码，我们为该特定列中的每个类别分配唯一的数值。 还有另一种方法可以很好地工作，称为**虚拟化或热编码**。 让我们借助一个例子来理解这一点。 让我们在现有的示例数据中再添加一列。 假设我们有一个包含用户使用的搜索引擎的添加列。

| Sr.NO | Time Spent | Search Engine | Converted |
| ----- | ---------- | ------------- | --------- |
| 1     | 5          | Google        | 0         |
| 2     | 2          | Bing          | 0         |
| 3     | 10         | Yahoo         | 1         |
| 4     | 15         | Bing          | 1         |
| 5     | 1          | Yahoo         | 0         |
| 6     | 12         | Google        | 1         |

​	因此，要使用搜索引擎列中提供的其他信息，我们必须使用dummification将其转换为数字格式。 因此，我们将获得额外数量的虚拟变量（列），这将等于“搜索引擎”列中不同类别的数量。 以下步骤解释了将分类特征转换为数字特征的整个过程。

*  找出分类列中不同数量的类别。 截至目前，我们只有三个不同的类别（Google，Bing，Yahoo）。
* 为每个不同类别创建新列，并在其所在的类别列中添加值1，否则为0

*Table.  One Hot encoding*

| Sr.No | Time Spent | Search Engine | SE_Google | SE_Bing | SE_Yahoo | Converted |
| ----- | ---------- | ------------- | --------- | ------- | -------- | --------- |
| 1     | 1          | Google        | 1         | 0       | 0        | 0         |
| 2     | 2          | Bing          | 0         | 1       | 0        | 0         |
| 3     | 5          | Yahoo         | 0         | 0       | 1        | 0         |
| 4     | 15         | Bing          | 0         | 1       | 0        | 1         |
| 5     | 17         | Yahoo         | 0         | 1       | 0        | 1         |
| 6     | 18         | Google        | 1         | 0       | 0        | 1         |



* 删除原始类别列。 因此，数据集现在总共包含五列（不包括索引），因为我们有三个额外的虚拟变量

*Table. Dummification*

| Sr.No | Time Spent | SE_Google | SE_Bing | SE_Yahoo | Converted |
| ----- | ---------- | --------- | ------- | -------- | --------- |
| 1     | 1          | 1         | 0       | 0        | 0         |
| 2     | 2          | 0         | 1       | 0        | 0         |
| 3     | 5          | 0         | 0       | 1        | 0         |
| 4     | 15         | 0         | 1       | 0        | 1         |
| 5     | 17         | 0         | 1       | 0        | 1         |
| 6     | 18         | 1         | 0       | 0        | 1         |

​	整个想法是以不同的方式表示相同的信息，以便机器学习模型也可以从分类值中学习。



## 5.4 模型评估

​	为了衡量逻辑回归模型的性能，我们可以使用多个指标。 最明显的是精度参数。 准确度是模型所做的正确预测的百分比。 但是，准确性并不总是首选方法。 要理解逻辑模型的性能，我们应该使用混淆矩阵。 它由预测值与实际值的值计数组成。 二进制类的混淆矩阵如表5-6所示。

| Actual/Prediction | Predicted Class(Yes) | Predicted Class(No) |
| ----------------- | -------------------- | ------------------- |
| Actual Class(Yes) | True Positives(TP)   | False Negative(FN)  |
| Actual Class(No)  | False Positives(FP)  | True Negatives(TN)  |



###   True Positives(TP)

​	实际上正类的值，模型也正确地预测是正类。

* Actual Class: Positive(1)
* ML Model Prediction Class: Positive(1)



### True Negatives(TN)

​	实际上为负类的值，模型也正确地预测是负类。

* Actual Class: Negative (0)
* ML Model Prediction Class: Negative (0)



### False Positives(FP)

​	实际上负面类别的值，但模型错误地预测它们是正类。

* Actual Class: Negative (0)
* ML Model Prediction Class: Positive (1)



### False Negatives(FN)

​	实际上正类的值，但模型错误地预测它们是负类。

* Actual Class: Positive (1)
* ML Model Prediction Class: Negative (0)



### Accuacy 

​	准确度是真阳性和真阴性的总和除以记录总数



### Recall

​	召回率有助于从正分类角度评估模型的性能。 它告诉模型能够在阳性病例总数中正确预测的实际阳性病例的百分比。

​			TP / (TP + FN)

​	它谈到了预测积极阶级时机器学习模型的质量。 因此，在总体正面类别中，模型能够正确预测多少？ **该度量被广泛用作分类模型的评估标准**。



### Precision

​	精确度是模型预测的所有正面案例中实际正面案例的数量：

​			TP / (TP + FP)



### F1 Score

​	f1 score = 2 * (Precision * Recall)/(Precision + Recall)





## 5.5 逻辑回归代码



### Step 1: Create the Spark Session Object

​	创建一个SparkSession对象

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('log_reg').getOrCreate()
```



### Step 2: Read the Dataset

​	读取数据

```python
df = spark.read.csv('Log_Reg_dataset.csv', inferSchema=True, header=True)
```



### Step 3: Exploratory Data Analysis

​	探索性数据分析。获取数据的形状(长、宽)

```python
print((df.count(), len(df.columns)))
```



​	所有列名

```python
[In]: df.printSchema()
[Out]: root
	|-- Country: string (nullable = true)
	|-- Age: integer (nullable = true)
	|-- Repeat_Visitor: integer (nullable = true)
	|-- Search_Engine: string (nullable = true)
	|-- Web_pages_viewed: integer (nullable = true)
	|-- Status: integer (nullable = true)
```

​	我们可以看到，有两个这样的列(Country，Search_Engine)，它们本质上是分类的，因此需要转换成数字形式。 让我们使用Spark中的`show`函数查看数据集。

```python
df.show(5)
```



​	输出所有数据的统计信息

```
df.describe().show()
```



​	使用`groupBy`函数可以对某个列进行聚合计算。

```python
df.groupBy('Country').count().show()
df.groupBy('Search_Engine').count().show()
df.groupBy('Status').count().show()
df.groupBy('Country').mean().show()
df.groupBy('Search_Engine').mean().show()
df.groupBy(Status).mean().show()
```



### Step 4: Feature Engineering

​	特征工程

​	这部分我们将分类变量转换为数字形式，并使用`Spark`的`VectorAssembler`创建一个整合了所有输入特征的单个向量。

```python
[In]: from pyspark.ml.feature import StringIndexer
[In]: from pyspark.ml.feature import VectorAssembler
```

​	由于我们处理的是两个分类列，因此我们必须将国家/地区和搜索引擎列转换为`数字形式`。 机器学习模型无法理解分类值。

​	第一步是使用`StringIndexer`将列标记为数字形式。 它为列的每个类别分配唯一值。 因此，在下面的示例中，搜索引擎（Yahoo，Google，Bing）的所有三个值都分配了值（0.0, 1.0, 2.0）。 这在名为`search_engine_num`的列中可见。

```python
[In]: search_engine_indexer = StringIndexer(inputCol="Search_Engine",        		outputCol="Search_Engine_Num").fit(df)
[In]: df = search_engine_indexer.transform(df)
[In]: df.show(3,False)
[Out]:
    +-------+---+-------------+----------------+-------+-----------------+
    |Country|Age|Repeat_Vistor|Web_pages_viewed|Status |Search_Engine_num|
    +-------+---+-------------+----------------+-------+-----------------+
    |India  |41 |1            |Yahoo           |1	   |0.0				 |
    |Brazil |28 |1            |Yahoo           |0	   |0.0				 |
    |Brazil |40 |0			  |Google		   |0	   |1.0				 |
    +-------+---+-------------+----------------+-------+-----------------+
    
[In]: df.groupBy('Search_Engine').count().orderBy('count',
                                                  ascending=False).show(5,False)
    # 根据'Search_Engine' 分组求和，再降序排序
[Out]:
    +--------------+-----+
    |Search_Engine |count|
    +--------------+-----+
    |Yahoo         |9859 |
    |Google        |5781 |
    |Bing		   |4360 |
    +--------------+-----+

[In]: df.groupBy('Search_Engine_Num').count().orderBy('count',
                                              ascending=False).show(5,False)
    # 根据'Search_Engine_Num' 分组求和，再降序排序
[Out]:
    +------------------+-----+
    |Search_Engine_Num |count|
    +------------------+-----+
    |0.0               |9859 |
    |1.0               |5781 |
    |2.0		       |4360 |
    +------------------+-----+
```

​	下一步是将这些值中的每一个表示为独热编码的形式。 然而，该向量在表示方面略有不同，因为它捕获向量中值的值和位置。

```python
[In]: from pyspark.ml.feature import OneHotEncoder
[In]: search_engine_encoder=OneHotEncoder(inputCol="Search_
Engine_Num", outputCol="Search_Engine_Vector")
[In]: df = search_engine_encoder.transform(df)
[In]: df.show(3,False)
[Out]:
    +-------+---+-------------+----------------+------+-----------------+-------------+
    |Country|Age|Repeat_Vistor|Web_pages_viewed|Status|Search_Engine_num|S_E_Vector   |
    +-------+---+-------------+----------------+------+-----------------+-------------+
    |India  |41 |1            |Yahoo           |1	  |0.0				|(2,[0],[1.0])|
    |Brazil |28 |1            |Yahoo           |0	  |0.0				|(2,[0],[1.0])|
    |Brazil |40 |0			  |Google		   |0	  |1.0			    |(2,[1],[1.0])|
    +-------+---+-------------+----------------+------+-----------------+-------------+ [In]: df.groupBy('Search_Engine_Vector').count().orderBy(
        'count',ascending=False).show(5,False)             
[Out]:                                          
    +------------------+-----+
    |Search_Engine_Vec |count|
    +------------------+-----+
    |(2,[0],[1.0])     |9859 |
    |(2,[1],[1.0])     |5781 |
    |(2,[],[])         |4360 |
    +------------------+-----+                                     
```



```python
# 理解下(2,[0],[1,0])的含义
(2,[0],[1,0]) 代表一个长度为2的向量，有一个值(1.0)
(1.0)的位置是第零位
```

​	

​	这种表示允许节省计算空间并因此更快地计算。向量的长度等于元素总数的一个，因为只需两列的帮助就可以很容易地表示每个值。例如，如果我们需要使用一个独热码代表搜索引擎，通常，我们可以如下所示进行。

| Search Engine | Google | Yahoo | Bing |
| ------------- | ------ | ----- | ---- |
| Google        | 1      | 0     | 0    |
| Yahoo         | 0      | 1     | 0    |
| Bing          | 0      | 0     | 1    |

​	另一种优化方式用两列替代三列表示。

| Search Engine | Google | Yahoo |
| ------------- | ------ | ----- |
| Google        | 1      | 0     |
| Yahoo         | 0      | 1     |
| Bing          | 0      | 0     |

```python
[In]: country_indexer = StringIndexer(inputCol="Country",
                                      outputCol="Country_Num").fit(df)
[In]: df = country_indexer.transform(df)
[In]: df.groupBy('Country').count().orderBy('count',ascending=False).show(5,False)
[Out]:
    +------------+-----+
    |Country     |count|
    +------------+-----+
    |Indonesia   |9859 |
    |India       |5781 |
    |Brazil      |4360 |
    |Malaysia    |1218 |
    +------------+-----+  

[In]: df.groupBy('Country_Num').count().orderBy('count',ascending=False).show(5,False)
[Out]:
    +------------+-----+
    |Country_Num |count|
    +------------+-----+
    |0.0         |9859 |
    |1.0         |5781 |
    |2.0         |4360 |
    |3.0         |1218 |
    +------------+-----+ 
    
[In]: country_encoder = OneHotEncoder(inputCol="Country_Num",
                                      outputCol="Country_Vector")
[In]: df = country_encoder.transform(df)
[In]: df.select(['Country','Country_Num','Country_Vector']).show(3,False)
[Out]:
    +----------+------------+--------------+
    |Country   |Country_Num |Country_Vector|
    +----------+------------+--------------+
    |India     |0.0         |(3,[1],[1.0]) |
    |Brazil    |2.0         |(3,[2],[1.0]) |
    |Brazil    |2.0         |(3,[2],[1.0]) |
    +----------+------------+--------------+ 
[In]: df.groupBy('Country_Vector').count().orderBy('count',
                                                   ascending=False).show(5,False)
[Out]:
    +---------------+-----+
    |Country_Vector |count|
    +---------------+-----+
    |(3,[0],[1.0])  |12178|
    |(3,[1],[1.0])  |4018 |
    |(3,[2],[1.0])  |2586 |
    |(3,[],[])      |1218 |
    +---------------+-----+ 
```

​	现在我们已经将分类列转换为数字形式，我们需要将所有输入列组合成一个矢量，该矢量将充当模型的输入特征。



```python
[In]: df_assembler = VectorAssembler(inputCols=['Search_Engine_Vector',
				'Country_Vector',
				'Age',
                'Repeat_Visitor',
                'Web_pages_viewed'],outputCol="features")
[In]: df = df_assembler.transform(df)
[In]: df.printSchema()
[Out]:
    root
     |-- Country: string (nullable = true)
     |-- Age: integer (nullable = true)
     |-- Repeat_Visitor: integer (nullable = true)
     |-- Search_Engine: string (nullable = true)
     |-- Web_pages_viewed: integer (nullable = true)
     |-- Status: integer (nullable = true)
     |-- Search_Engine_Num: double (nullable = false)
     |-- Search_Engine_Vector: vector (nullable = true)
     |-- Country_Num: double (nullable = false)
     |-- Country_Vector: vector (nullable = true)
     |-- features: vector (nullable = true)
```

​	可以看到，得到了一个额外列(features)，它作为一个单独的稠密向量包含了所有输入特征。



```python
[In]: df.select(['features','Status']).show(10,False)
[Out]:
    +-----------------------------------+------+
    |features							|Status|
    +-----------------------------------+------+
    |[1.0,0.0,0.0,1.0,0.0,41.0,1.0,21.0]|1	   |
    |[1.0,0.0,0.0,0.0,1.0,28.0,1.0,5.0] |0	   |
    |(8,[1,4,5,7],[1.0,1.0,40.0,3.0])   |0	   |
    |(8,[2,5,6,7],[1.0,31.0,1.0,15.0])  |1	   |
    |(8,[1,5,7],[1.0,32.0,15.0]) 	    |1	   |
    |(8,[1,4,5,7],[1.0,1.0,32.0,3.0])   |0	   |
    |(8,[1,4,5,7],[1.0,1.0,32.0,6.0])   |0	   |
    |(8,[1,2,5,7],[1.0,1.0,27.0,9.0])   |0	   |
    |(8,[0,2,5,7],[1.0,1.0,32.0,2.0])   |0	   |
    |(8,[2,5,6,7],[1.0,31.0,1.0,16.0])  |1	   |
    +-----------------------------------+------+
```

​	让我们只选择要素列作为输入，并选择`Status`列作为训练逻辑回归模型的输出。

```python
[In]: model_df=df.select(['features','Status'])
```



### Step 5: Splitting the Dataset

​	切分数据集

​	我们必须将数据集拆分为训练和测试数据集，以便训练和评估逻辑回归模型的性能。我们将它以75/25的比例分割，并在75％的数据集上训练我们的模型。分割数据的另一个用途是我们可以使用75％的数据来应用交叉验证，以便提出最好的超参数。交叉验证可以是不同类型，其中一部分训练数据被保留用于训练，而剩余部分用于验证目的。K折交叉验证主要用于训练具有最佳超参数的模型。

​	输出训练集和测试集的大小。

```python
[In]: training_df,test_df = model_df.randomSplit([0.75,0.25])
[In]: print(training_df.count())
[Out]: (14907)
    
[In]: training_df.groupBy('Status').count().show()
[Out]:
        +------+-----+
        |Status|count|
        +------+-----+
        | 1    | 7417|
        | 0    | 7490|
        +------+-----+
```

​	这可确保我们在训练和测试集中有一个平衡的目标类（Status）集合。

```python
[In]:print(test_df.count())
[Out]: (5093)
    
[In]: test_df.groupBy('Status').count().show()
[Out]:
    +------+-----+
    |Status|count|
    +------+-----+
    | 1    | 2583|
    | 0    | 2510|
    +------+-----+
```



### Step 6: Build and Train Logistic Regression Model

​	在这一部分中，我们使用特征作为输入列和状态作为输出列来构建和训练逻辑回归模型。

```python
[In]: from pyspark.ml.classification import LogisticRegression
[In]: log_reg=LogisticRegression(labelCol='Status').fit(training_df)
```



### Training Results

​	我们可以使用`Spark中`的`evaluate`函数获得模型所做的预测，以优化的方式执行所有步骤。 这使得另一个`Dataframe`总共包含四列，包括`预测`和`概率`。 `预测列`表示模型已针对给定行预测的类标签，并且概率列包含两个概率（第0个索引处的负类概率和第1个索引处的正类概率）。

```python
[In]: train_results=log_reg.evaluate(training_df).predictions
[In]: train_results.filter(
    	train_results['Status']==1).filter(
    	train_results['prediction']==1).select(['Status',
                                            'prediction',
                                            'probability']).show(10,False)
[Out]:
+------+----------+----------------------------------------+
|Status|prediction|probability 							   |
+------+----------+----------------------------------------+
|1	   |1.0		  |[0.2978572628475072,0.7021427371524929] |
|1	   |1.0		  |[0.2978572628475072,0.7021427371524929] |
|1	   |1.0		  |[0.16704676975730415,0.8329532302426959]|
|1	   |1.0		  |[0.16704676975730415,0.8329532302426959]|
|1	   |1.0		  |[0.16704676975730415,0.8329532302426959]|
|1	   |1.0		  |[0.08659913656062515,0.9134008634393749]|
|1	   |1.0		  |[0.08659913656062515,0.9134008634393749]|
|1	   |1.0		  |[0.08659913656062515,0.9134008634393749]|
|1	   |1.0		  |[0.08659913656062515,0.9134008634393749]|
|1	   |1.0		  |[0.08659913656062515,0.9134008634393749]|
+------+----------+----------------------------------------+
```

​	因此，在上述结果中，第0个索引的概率是Status = 0，第1个索引的概率是Status = 1。



### Step 7: Evaluate Linear Regression Model on Test Data
​	整个练习的最后一部分是检查模型在看不见或测试数据上的表现。 我们再次利用`evaluate`函数对测试进行预测。
​	我们将预测`DataFrame`分配给结果和结果`DataFrame`现在包含五列。

```python
[In]: results=log_reg.evaluate(test_df).predictions
[In]: results.printSchema()
[Out]:
      root
        |-- features: vector (nullable = true)
        |-- Status: integer (nullable = true)
        |-- rawPrediction: vector (nullable = true)
        |-- probability: vector (nullable = true)
        |-- prediction: double (nullable = false)
```

​	我们可以使用`select`关键字过滤我们想要查看的列。

```python
[In]: results.select(['Status','prediction']).show(10,False)
[Out]:
        +------+----------+
        |Status|prediction|
        +------+----------+
        |0     |0.0	      |
        |0     |0.0	      |
        |0     |0.0	      |
        |0     |0.0	      |
        |1     |0.0	      |
        |0     |0.0	      |
        |1     |1.0	      |
        |0     |1.0	      |
        |1     |1.0	      |
        |1     |1.0	      |
		+------+----------+
```

​	由于这是一个分类问题，我们将使用混淆矩阵来衡量模型的性能。



### Confusion Matrix

​	我们将手动创建`true positives, true negatives, false positives, false negatives`的变量，以便更好地理解它们，而不是使用直接内置函数。

```python
[In]:tp = results[(results.Status == 1) & (results.prediction == 1)].count()
[In]:tn = results[(results.Status == 0) & (results.prediction == 0)].count()
[In]:fp = results[(results.Status == 0) & (results.prediction == 1)].count()
[In]:fn = results[(results.Status == 1) & (results.prediction == 0)].count()
```



#### Accuracy

​	正如本章已经讨论的那样，准确性是评估任何分类器的最基本指标; 但是，由于依赖于目标类平衡，这不是模型性能的正确指标。

(TP + TN) / (TP + TN + FP + FN)

```python
[In]: accuracy=float((true_postives+true_negatives) /(results.count()))
[In]: print(accuracy)
[Out]: 0.9374255065554231
```



#### Recall

​	召回率显示我们能够在总体积极观察中正确预测多少阳性类别案例。

​	(TP)/(TP + FN)

```python
[In]: recall = float(true_postives)/(true_postives + false_negatives)
[In]: print(recall)
[Out]: 0.937524870672503
```



#### Precision

​	(TP)/(TP + FP)

​	精确率描述了所有预测的阳性观察中正确预测的真阳性数：

```python
[In]: precision = float(true_postives) / (true_postives + false_positives)
[In]: print(precision)
[Out]: 0.9371519490851233
```



## 总结

​	在本章中，我们讨论了理解逻辑回归的构建块，将分类列转换为数字特征，以及使用`PySpark`从头开始构建逻辑回归模型的过程。